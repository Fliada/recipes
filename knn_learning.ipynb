{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-08T20:26:51.800057600Z",
     "start_time": "2023-07-08T20:26:51.215818100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\zxc\n",
      "[nltk_data]     ghoul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve, accuracy_score,hamming_loss\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import faiss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T18:25:26.090338600Z",
     "start_time": "2023-07-08T18:25:26.003140100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T19:30:18.820356100Z",
     "start_time": "2023-07-08T19:30:15.160605100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.4\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "try:\n",
    "    faiss_version = pkg_resources.get_distribution(\"faiss\").version\n",
    "    print(faiss_version)\n",
    "except pkg_resources.DistributionNotFound:\n",
    "    print(\"FAISS is not installed.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T18:25:26.105343800Z",
     "start_time": "2023-07-08T18:25:26.094341700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '4', '5', '7', '8', '14', '16', '17', '20', '25', '27', '30', '33', '37', '43', '51', '62', '67', '68', '71', '90', '99', '103', '105', '139', '140', '150', '158', '257']\n"
     ]
    }
   ],
   "source": [
    "db_name = 'recipe.db'\n",
    "conn = sqlite3.connect(db_name)\n",
    "\n",
    "table_name = 'recipe'\n",
    "\n",
    "#забираем доступные категории\n",
    "categories_sql = f\"select c.id, c.name_cat from category c \" \\\n",
    "                 f\"join recipe_categories rc on \" \\\n",
    "                 f\"c.id = rc.cat_id \" \\\n",
    "                 f\"GROUP by c.id \" \\\n",
    "                 f\"HAVING count(*) > 100 \" \\\n",
    "                 f\"ORDER BY c.id\"\n",
    "\n",
    "loaded_categories = pd.read_sql(categories_sql, conn)\n",
    "\n",
    "ids_cat = []\n",
    "for i in range(len(loaded_categories)):\n",
    "    ids_cat.append(str(loaded_categories.id[i]))\n",
    "\n",
    "print(ids_cat)\n",
    "\n",
    "#забираем способ приготовления рецепта с категориями из доступных с id категорий\n",
    "sql = f\"SELECT r.manual, (SELECT group_concat(rc.cat_id , ', ') \" \\\n",
    "      f\"from recipe_categories rc \" \\\n",
    "      f\"WHERE rc.recipe_id = r.id AND \" \\\n",
    "      f\"rc.cat_id in ({', '.join(ids_cat)})) \" \\\n",
    "      f\"AS categories from recipe r\"\n",
    "\n",
    "loaded_data = pd.read_sql(sql, conn)\n",
    "\n",
    "#pandas data frame\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T18:25:46.883076600Z",
     "start_time": "2023-07-08T18:25:26.110474300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "for i in range(len(ids_cat)):\n",
    "    val = []\n",
    "    for j in range(len(loaded_data)):\n",
    "        spl = str(loaded_data.categories[j]).split(', ')\n",
    "        #print(spl)\n",
    "        #print(int(str(ids_cat[i]) in spl))\n",
    "        val.append(int(str(ids_cat[i]) in spl))\n",
    "    loaded_data.insert(loc=len(loaded_data.columns) , column=ids_cat[i], value=val)\n",
    "#print(val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T18:25:49.801556700Z",
     "start_time": "2023-07-08T18:25:46.897780300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 manual   categories  1  2  4  \\\n0     Шоколад разломать на кусочки и вместе со сливо...         1, 2  1  1  0   \n1     Положите весь творог в кастрюльку и разомните ...         4, 5  0  0  1   \n2     Вскипятите воду в большой кастрюле и сварите п...         7, 8  0  0  0   \n3     Разогреть духовку. Отделить белки от желтков. ...         1, 5  1  0  0   \n4     Взбить яйца с сахаром.\\n\\nПостепенно ввести му...     1, 5, 14  1  0  0   \n...                                                 ...          ... .. .. ..   \n6990  Печенку посолить, поперчить, посыпать специями...        30, 5  0  0  0   \n6991  Крабовые палочки мелко нарезать или порубить в...      17, 140  0  0  0   \n6992  Чечевицу промыть, залить водой на 4 см выше ур...           37  0  0  0   \n6993  Нарезать филе средними полосками. Морковь наре...  17, 67, 139  0  0  0   \n6994  Очищенный и вымытый картофель нарезать кубикам...           37  0  0  0   \n\n      5  7  8  14  16  ...  71  90  99  103  105  139  140  150  158  257  \n0     0  0  0   0   0  ...   0   0   0    0    0    0    0    0    0    0  \n1     1  0  0   0   0  ...   0   0   0    0    0    0    0    0    0    0  \n2     0  1  1   0   0  ...   0   0   0    0    0    0    0    0    0    0  \n3     1  0  0   0   0  ...   0   0   0    0    0    0    0    0    0    0  \n4     1  0  0   1   0  ...   0   0   0    0    0    0    0    0    0    0  \n...  .. .. ..  ..  ..  ...  ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  \n6990  1  0  0   0   0  ...   0   0   0    0    0    0    0    0    0    0  \n6991  0  0  0   0   0  ...   0   0   0    0    0    0    1    0    0    0  \n6992  0  0  0   0   0  ...   0   0   0    0    0    0    0    0    0    0  \n6993  0  0  0   0   0  ...   0   0   0    0    0    1    0    0    0    0  \n6994  0  0  0   0   0  ...   0   0   0    0    0    0    0    0    0    0  \n\n[6995 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>manual</th>\n      <th>categories</th>\n      <th>1</th>\n      <th>2</th>\n      <th>4</th>\n      <th>5</th>\n      <th>7</th>\n      <th>8</th>\n      <th>14</th>\n      <th>16</th>\n      <th>...</th>\n      <th>71</th>\n      <th>90</th>\n      <th>99</th>\n      <th>103</th>\n      <th>105</th>\n      <th>139</th>\n      <th>140</th>\n      <th>150</th>\n      <th>158</th>\n      <th>257</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Шоколад разломать на кусочки и вместе со сливо...</td>\n      <td>1, 2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Положите весь творог в кастрюльку и разомните ...</td>\n      <td>4, 5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Вскипятите воду в большой кастрюле и сварите п...</td>\n      <td>7, 8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Разогреть духовку. Отделить белки от желтков. ...</td>\n      <td>1, 5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Взбить яйца с сахаром.\\n\\nПостепенно ввести му...</td>\n      <td>1, 5, 14</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6990</th>\n      <td>Печенку посолить, поперчить, посыпать специями...</td>\n      <td>30, 5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6991</th>\n      <td>Крабовые палочки мелко нарезать или порубить в...</td>\n      <td>17, 140</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6992</th>\n      <td>Чечевицу промыть, залить водой на 4 см выше ур...</td>\n      <td>37</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6993</th>\n      <td>Нарезать филе средними полосками. Морковь наре...</td>\n      <td>17, 67, 139</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6994</th>\n      <td>Очищенный и вымытый картофель нарезать кубикам...</td>\n      <td>37</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>6995 rows × 32 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T18:34:23.160030Z",
     "start_time": "2023-07-08T18:34:23.093597400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#делим dataset на тренировочный и тестовый\n",
    "X_train,X_test,y_train,y_test = train_test_split(loaded_data[\"manual\"], loaded_data[ids_cat],test_size=0.3,random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T18:25:49.854113700Z",
     "start_time": "2023-07-08T18:25:49.820692800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer(language=\"russian\")\n",
    "russian_stop_words = stopwords.words(\"russian\")\n",
    "\n",
    "def tokenize_sentence(sentence: str, remove_stop_words: bool = True):\n",
    "    #print(sentence)\n",
    "    tokens = word_tokenize(sentence, language=\"russian\")\n",
    "    tokens = [i for i in tokens if i not in string.punctuation]\n",
    "    if remove_stop_words:\n",
    "        tokens = [i for i in tokens if i not in russian_stop_words]\n",
    "    tokens = [snowball.stem(i) for i in tokens]\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T22:44:24.987737300Z",
     "start_time": "2023-07-08T22:44:24.969519200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "class Tokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, remove_stop_words=True):\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        self.stop_words = set(stopwords.words('russian'))\n",
    "        self.stemmer = SnowballStemmer('russian')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [self.tokenize_sentence(sentence) for sentence in X]\n",
    "\n",
    "    def tokenize_sentence(self, sentence):\n",
    "        tokens = word_tokenize(sentence, language=\"russian\")\n",
    "        tokens = [i for i in tokens if i not in string.punctuation]\n",
    "        if self.remove_stop_words:\n",
    "            tokens = [i for i in tokens if i not in self.stop_words]\n",
    "        tokens = [self.stemmer.stem(i) for i in tokens]\n",
    "        return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T20:27:42.518730800Z",
     "start_time": "2023-07-08T20:27:42.504295400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "class BertVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name='cointegrated/rubert-tiny'):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.max_sequence_length = 510\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        vectors = []\n",
    "        for tokens in X:\n",
    "            tokens = tokens[:self.max_sequence_length]  # Ограничить длину последовательности до 512 токенов\n",
    "            tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "            input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            input_tensor = torch.tensor(input_ids).unsqueeze(0).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_tensor)\n",
    "                embeddings = outputs[0]\n",
    "\n",
    "            text_vector = embeddings.squeeze().mean(dim=0).cpu().numpy()\n",
    "            vectors.append(text_vector)\n",
    "\n",
    "        return vectors"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T08:22:59.733215700Z",
     "start_time": "2023-07-09T08:22:59.704222700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "class BertVectorizer2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('cointegrated/rubert-tiny')\n",
    "        self.model = BertModel.from_pretrained('cointegrated/rubert-tiny')\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        vectors = []\n",
    "        for text in X:\n",
    "            text_vector = embed_bert_cls(text, model=self.model, tokenizer=self.tokenizer)\n",
    "            vectors.append(text_vector)\n",
    "\n",
    "        return vectors"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T08:42:56.337507200Z",
     "start_time": "2023-07-09T08:42:56.327282500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "# model.cuda()  # uncomment it if you have a GPU\n",
    "\n",
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T08:43:00.602062900Z",
     "start_time": "2023-07-09T08:42:59.780596800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-2.84503587e-02,  2.48572547e-02,  4.52326350e-02, -7.95979351e-02,\n",
      "       -1.53341535e-02,  1.95008032e-02,  6.84027076e-02, -2.58912821e-03,\n",
      "       -8.90688002e-02, -7.47816861e-02,  5.33769019e-02,  3.95353809e-02,\n",
      "        2.15761568e-02,  1.92577671e-02,  8.65388010e-03,  1.72012746e-02,\n",
      "        5.63668571e-02,  1.71543527e-02,  2.38393825e-02,  1.00879081e-01,\n",
      "       -3.04934266e-03, -1.82165317e-02,  1.61965471e-02,  1.45853544e-02,\n",
      "        8.97004232e-02,  2.11753529e-02, -1.13322604e-02,  1.37898242e-02,\n",
      "       -1.44742774e-02,  5.83038032e-02, -9.02313739e-02, -7.45823830e-02,\n",
      "        1.09646395e-01,  2.36695185e-02, -2.57688314e-02,  8.15490335e-02,\n",
      "        1.42579060e-03, -1.82034243e-02, -1.73426896e-01, -3.17930169e-02,\n",
      "       -7.68856108e-02,  8.63788426e-02,  1.19000688e-01,  5.18519012e-03,\n",
      "       -2.13312805e-02,  4.75826524e-02,  3.57612483e-02, -1.77896786e-02,\n",
      "        7.05813840e-02, -6.07864782e-02,  2.48750187e-02, -5.70905954e-02,\n",
      "        1.31985229e-02, -8.18155855e-02, -1.96225256e-01,  8.16287324e-02,\n",
      "        1.14382364e-01,  3.10905240e-02, -5.76074347e-02,  1.15664182e-02,\n",
      "        5.26937880e-02,  6.90283850e-02, -2.31972840e-02,  4.41516563e-02,\n",
      "        2.62674671e-02,  1.14292949e-01, -6.32378682e-02,  3.22253741e-02,\n",
      "       -8.95162523e-02, -6.74475404e-03,  3.24033387e-02,  3.35392505e-02,\n",
      "       -3.69854853e-03, -6.33003339e-02, -5.10212183e-02,  5.02248704e-02,\n",
      "       -3.31195928e-02,  1.75971761e-02, -1.73394922e-02,  2.08459850e-02,\n",
      "        7.70587358e-04,  4.31425124e-02,  3.22565995e-03,  4.53968830e-02,\n",
      "       -6.94160862e-03,  3.01019344e-02,  4.04444942e-03,  2.66366079e-03,\n",
      "        3.16147432e-02,  7.20456894e-03,  8.43254775e-02,  2.94355415e-02,\n",
      "        1.61320999e-01, -5.84965805e-03, -6.10699458e-03, -8.94530211e-04,\n",
      "       -6.96614105e-03,  6.59585148e-02, -3.77252810e-02,  2.65637413e-02,\n",
      "       -1.58086512e-03, -1.39727127e-02, -2.91929655e-02, -6.42752126e-02,\n",
      "       -2.87014656e-02, -7.98034146e-02, -3.65799367e-02, -1.13343792e-02,\n",
      "       -7.80004542e-03, -4.65919711e-02, -3.83697264e-02,  1.63822412e-03,\n",
      "        1.53704807e-01,  1.61749981e-02,  1.30760521e-02,  4.99596559e-02,\n",
      "       -5.02885282e-02,  5.96323377e-03,  3.69479991e-02, -2.79659871e-02,\n",
      "        3.57423872e-02, -8.56462270e-02, -2.78317612e-02,  5.85559532e-02,\n",
      "       -4.96230274e-03, -4.68837954e-02, -2.03249324e-02,  2.88389772e-02,\n",
      "       -1.25456266e-02,  2.34149070e-03, -7.97887221e-02,  1.31512352e-03,\n",
      "       -4.61475924e-03,  5.88650405e-02,  8.49350262e-03,  3.24132070e-02,\n",
      "       -8.58994760e-03,  3.91250513e-02,  4.54984866e-02, -8.77271146e-02,\n",
      "        3.25833373e-02, -1.67344324e-02,  6.22783750e-02,  9.90382116e-03,\n",
      "        6.61569973e-03, -4.43703830e-02,  8.64839852e-02,  3.12102865e-02,\n",
      "       -1.63765941e-02,  3.24640200e-02,  3.46214436e-02,  2.14679260e-02,\n",
      "       -3.80119681e-02, -6.08616993e-02,  9.38823223e-02, -1.37324596e-03,\n",
      "       -2.23685279e-02,  3.07207531e-03, -3.06902174e-02,  2.46562380e-02,\n",
      "        3.73472422e-02,  4.48510759e-02,  2.62873620e-02,  3.51065025e-02,\n",
      "        2.48406343e-02, -4.12904546e-02, -1.98904611e-03,  2.56045144e-02,\n",
      "       -3.75545770e-02, -1.49155485e-05,  8.95954520e-02,  1.02321804e-01,\n",
      "       -7.75177479e-02,  5.15677780e-02,  4.07728329e-02,  5.56997508e-02,\n",
      "       -3.12727839e-02, -3.34161408e-02,  1.80469155e-02,  4.24158387e-02,\n",
      "        7.18013793e-02, -3.27488594e-02,  3.12250182e-02, -6.41526934e-03,\n",
      "        8.63311440e-02,  5.06936014e-02, -9.63731110e-03, -3.86482999e-02,\n",
      "        3.45476437e-03,  3.35999355e-02, -2.39134729e-02,  1.36868693e-02,\n",
      "       -8.02229866e-02,  4.10970449e-02,  1.11314841e-01, -3.55650365e-01,\n",
      "        3.94797884e-03,  1.70912489e-01, -1.45032573e-02,  8.81148037e-03,\n",
      "       -1.06110433e-02,  1.01862121e-02,  9.01323371e-03, -1.15732811e-02,\n",
      "        1.46312684e-01, -6.37163445e-02,  1.09314390e-01,  1.14358012e-02,\n",
      "       -3.11158989e-02, -4.69978191e-02, -2.59855166e-02, -8.49663690e-02,\n",
      "       -4.38420512e-02,  4.72506881e-02, -2.97210477e-02, -1.45604894e-01,\n",
      "        5.92074133e-02, -5.23197129e-02, -2.25067213e-02, -2.97205709e-02,\n",
      "        4.00350168e-02,  4.96220328e-02, -3.47311795e-02,  2.78546307e-02,\n",
      "        4.49340306e-02, -1.02680728e-01, -3.69548202e-02,  4.15183417e-02,\n",
      "       -9.66564789e-02,  4.40157764e-02, -7.28418082e-02, -7.17696175e-02,\n",
      "       -8.07460472e-02, -1.86234328e-03,  5.74870110e-02, -1.06296819e-02,\n",
      "       -3.34221832e-02, -5.00220433e-02, -1.40914377e-02, -1.98500026e-02,\n",
      "       -1.04174409e-02,  2.58647986e-02, -9.68018454e-03, -8.64649266e-02,\n",
      "       -3.19865458e-02,  1.87926963e-02,  9.55740083e-03, -3.65840159e-02,\n",
      "       -2.37423871e-02,  2.18216535e-02, -6.36352226e-02,  1.54253244e-02,\n",
      "       -6.46219552e-02, -5.57894334e-02,  2.83598714e-02, -5.67667671e-02,\n",
      "       -8.69948566e-02, -2.59791147e-02, -2.23351792e-02,  3.23204547e-02,\n",
      "        4.89245169e-02,  5.32679297e-02,  2.21973769e-02,  1.11880582e-02,\n",
      "       -1.39666528e-01,  1.75623093e-02, -8.69991444e-03, -1.25347851e-02,\n",
      "        1.35753695e-02, -7.92744383e-03,  4.93885875e-02,  4.01302800e-02,\n",
      "       -6.34738505e-02,  6.13217913e-02,  3.79927605e-02,  1.71142928e-02,\n",
      "        3.35496292e-02, -3.18265185e-02,  3.28289829e-02, -6.28356338e-02,\n",
      "        6.32865131e-02,  2.53105294e-02, -3.59348729e-02,  8.13574567e-02,\n",
      "        2.14969441e-02, -3.26126069e-02, -4.46960777e-02,  3.44281569e-02,\n",
      "       -2.76517347e-02, -5.14193252e-02, -1.60971470e-02,  3.16531695e-02,\n",
      "        4.65124175e-02,  1.02381408e-02, -5.23232222e-02,  6.56787083e-02,\n",
      "       -7.38724247e-02, -8.46490730e-03, -2.52996236e-02, -1.81409623e-02,\n",
      "       -3.30633596e-02, -2.24119462e-02,  2.94833370e-02,  9.54997167e-02,\n",
      "       -1.13596413e-02, -2.32962258e-02, -5.22570089e-02, -4.04514484e-02,\n",
      "       -5.90167902e-02,  1.08546808e-01,  4.24223356e-02, -2.09014714e-02],\n",
      "      dtype=float32), array([-4.55471277e-02, -3.44848856e-02,  5.19589260e-02, -5.88598363e-02,\n",
      "       -4.29246128e-02,  1.20323040e-02,  8.13115314e-02,  1.45011162e-02,\n",
      "       -8.31745379e-03, -6.64897263e-02,  3.54009010e-02,  2.88440660e-03,\n",
      "       -1.97136146e-03,  7.98191205e-02,  6.33185191e-05,  6.28593285e-03,\n",
      "        2.93647684e-02, -1.29464716e-02,  3.56092080e-02,  1.32795319e-01,\n",
      "       -1.48867089e-02, -7.88585003e-03,  4.02050745e-03, -4.97745946e-02,\n",
      "        6.92290664e-02,  5.79006523e-02, -5.60076535e-02, -1.46972574e-02,\n",
      "       -4.63606883e-03,  4.39145342e-02, -1.16446763e-02, -4.14643362e-02,\n",
      "        3.27806100e-02,  3.20396274e-02, -5.18712820e-03,  7.41367638e-02,\n",
      "        1.97608229e-02, -6.36935532e-02, -1.41581133e-01, -1.87346619e-02,\n",
      "       -1.22182928e-02,  3.25520709e-02,  1.09123744e-01, -1.10749379e-02,\n",
      "        1.85863953e-02,  2.37850137e-02,  2.85354294e-02, -3.13258991e-02,\n",
      "        8.04232359e-02, -5.17981406e-03,  2.89914608e-02, -1.92376524e-02,\n",
      "       -1.90606024e-02, -2.18475685e-02, -1.58160359e-01,  8.44084024e-02,\n",
      "        5.71046248e-02,  2.74235886e-02, -1.32179838e-02, -1.49321597e-04,\n",
      "        7.50736967e-02, -1.53728910e-02,  8.48024711e-03,  3.10374163e-02,\n",
      "        1.77288540e-02,  1.10442474e-01, -4.82896306e-02,  4.90837432e-02,\n",
      "       -4.66072150e-02, -1.21355643e-02, -4.96276766e-02,  2.12063231e-02,\n",
      "        4.59070178e-03, -7.01860636e-02, -2.78769284e-02,  2.10868381e-03,\n",
      "        1.93602722e-02, -4.50083353e-02,  4.15835194e-02,  5.92880547e-02,\n",
      "       -3.84026542e-02, -7.88139645e-04, -3.72762978e-02,  7.04741627e-02,\n",
      "        1.50574530e-02, -2.88130492e-02, -3.22742313e-02,  4.10643360e-03,\n",
      "        4.10238057e-02,  2.67879553e-02,  1.23027027e-01,  2.70478372e-02,\n",
      "        2.47172579e-01, -1.37889422e-02,  3.66326724e-03, -1.90375298e-02,\n",
      "       -1.82801466e-02,  2.34730411e-02, -2.70099193e-02, -2.42538676e-02,\n",
      "       -2.61097904e-02,  2.12075803e-02,  1.13247475e-02, -2.37507932e-02,\n",
      "       -1.67640857e-02, -7.20202848e-02, -3.14835645e-03, -2.91920756e-03,\n",
      "       -1.49855968e-02, -1.67366620e-02, -3.09881903e-02,  1.27648050e-02,\n",
      "        1.51129916e-01,  9.80220502e-04,  2.85243131e-02,  2.62786467e-02,\n",
      "       -4.11652848e-02,  2.88772937e-02,  2.77831871e-02, -2.69940607e-02,\n",
      "        1.91967450e-02, -1.21720001e-01,  9.02441423e-03,  2.13246755e-02,\n",
      "       -2.50914670e-03, -1.79066341e-02, -3.87142822e-02,  5.08054625e-03,\n",
      "       -7.56080402e-03,  2.69558989e-02, -3.86714824e-02, -2.59552011e-03,\n",
      "        6.07171422e-03,  4.08934653e-02, -2.04620827e-02,  3.46936248e-02,\n",
      "        2.78250817e-02,  1.16985831e-02,  4.10256684e-02, -5.10665448e-03,\n",
      "        6.33690879e-03, -1.23583101e-01,  3.66563424e-02, -5.14211506e-03,\n",
      "       -3.96798626e-02, -8.06873813e-02, -3.05780601e-02,  1.11025258e-03,\n",
      "       -2.18735468e-02,  3.02689355e-02,  2.69473018e-03, -2.34173182e-02,\n",
      "       -4.57461327e-02, -5.23371398e-02,  8.77791271e-02, -1.18021742e-02,\n",
      "       -6.37133643e-02,  2.01178174e-02,  6.92411512e-03,  5.98377958e-02,\n",
      "        9.65751614e-03,  3.33792344e-02,  1.84050798e-02,  4.25804183e-02,\n",
      "        6.59769550e-02, -6.55258493e-03,  8.06560367e-03,  1.18927024e-02,\n",
      "       -1.98835842e-02, -1.99229978e-02,  5.30678742e-02,  2.66757645e-02,\n",
      "       -6.46214336e-02,  2.11406164e-02,  7.81280920e-03,  5.99379577e-02,\n",
      "        2.12576240e-02, -2.74594151e-03,  1.19072450e-02,  2.60242783e-02,\n",
      "        6.72522113e-02, -5.04661240e-02,  3.67235653e-02, -1.32504525e-02,\n",
      "        4.43560816e-02,  5.33326622e-03,  7.68853677e-03, -5.41704930e-02,\n",
      "       -7.73599092e-03,  6.73766881e-02,  6.19928027e-03,  1.85004587e-03,\n",
      "       -8.73101354e-02,  6.01919368e-03,  1.55415654e-01, -3.80521148e-01,\n",
      "        6.92958608e-02,  9.98763666e-02,  1.77514683e-02, -4.99385782e-02,\n",
      "        6.83510723e-03, -6.87946938e-03, -4.75851409e-02, -1.35631403e-02,\n",
      "        2.45510072e-01, -3.37381773e-02,  1.07737415e-01,  1.14972945e-02,\n",
      "       -1.84816923e-02, -4.21048477e-02,  3.67726907e-02, -3.74384299e-02,\n",
      "        1.59933325e-02,  7.34170154e-02, -2.37054937e-02, -1.30446911e-01,\n",
      "        4.14780937e-02, -2.04107594e-02, -5.36705069e-02,  1.17966477e-02,\n",
      "        3.18223424e-02,  2.69414485e-02, -3.47581040e-03,  2.03564484e-03,\n",
      "        2.55478173e-02, -1.15894526e-01,  1.79169083e-03, -5.05136326e-03,\n",
      "       -8.02425072e-02,  3.71207185e-02, -4.51729707e-02, -7.53690004e-02,\n",
      "       -6.77376837e-02,  1.06930006e-02,  2.86396127e-02, -1.39568252e-02,\n",
      "        7.83770680e-02, -7.16446564e-02, -1.95812583e-02, -3.39712426e-02,\n",
      "        2.51477323e-02,  2.63917167e-02,  1.31729664e-02, -7.28032961e-02,\n",
      "        1.46234892e-02,  1.61444973e-02,  2.51720324e-02,  2.04508118e-02,\n",
      "       -2.04021987e-02, -8.11412279e-03, -1.77731905e-02, -2.97756009e-02,\n",
      "       -1.15043662e-01, -1.10867403e-01,  4.98574926e-03, -1.89022962e-02,\n",
      "       -2.26977300e-02,  1.55291157e-02, -1.42456815e-02,  2.36709137e-02,\n",
      "        5.19427285e-02,  4.57867868e-02,  5.62031679e-02,  1.72484468e-03,\n",
      "       -1.41700938e-01,  4.07490693e-02, -1.79701429e-02, -3.71693186e-02,\n",
      "        3.70408818e-02, -2.86909454e-02,  5.40636256e-02,  7.07450286e-02,\n",
      "        1.44567648e-02,  1.15169585e-01, -1.02453111e-02,  1.38662020e-02,\n",
      "       -8.76389723e-03, -5.36968820e-02,  2.42934395e-02, -3.71282687e-03,\n",
      "        5.29557429e-02,  7.30189728e-03, -4.95512336e-02,  8.83210748e-02,\n",
      "        1.33841587e-02,  4.35107294e-03, -9.00984854e-02,  2.46354807e-02,\n",
      "        3.07395328e-02, -6.95917755e-02,  2.20942199e-02,  6.18955400e-03,\n",
      "        3.65951732e-02,  3.11823580e-02, -2.79439613e-02,  6.65480345e-02,\n",
      "       -1.31820962e-01,  2.17249915e-02, -1.27126276e-02, -9.43818539e-02,\n",
      "       -2.08282117e-02, -4.41364832e-02,  5.18056890e-03,  5.46141863e-02,\n",
      "       -2.44890042e-02, -1.98844634e-02,  4.31016944e-02, -1.61807928e-02,\n",
      "       -8.91951397e-02,  1.16546124e-01,  6.08631223e-02, -7.70018548e-02],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(BertVectorizer2().transform([\"привет мир\", \"awfawf\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T08:45:12.874479600Z",
     "start_time": "2023-07-09T08:45:12.112410400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Шоколад разломать на кусочки и вместе со сливочным маслом растопить на водяной бане, не переставая все время помешивать лопаткой или деревянной ложкой. Получившийся густой шоколадный соус снять с водяной бани и оставить остывать.\n",
      "\n",
      "Тем временем смешать яйца со ста граммами коричневого сахара: яйца разбить в отдельную миску и взбить, постепенно добавляя сахар. Взбивать можно при помощи миксера или вручную — как больше нравится, — но не меньше двух с половиной-трех минут.\n",
      "\n",
      "Острым ножом на разделочной доске порубить грецкие орехи. Предварительно их можно поджарить на сухой сковороде до появления аромата, но это необязательная опция.\n",
      "\n",
      "В остывший растопленный со сливочным маслом шоколад аккуратно добавить оставшийся сахар, затем муку и измельченные орехи и все хорошо перемешать венчиком.\n",
      "\n",
      "Затем влить сахарно-яичную смесь и тщательно смешать с шоколадной массой. Цвет у теста должен получиться равномерным, без разводов.\n",
      "\n",
      "Разогреть духовку до 200 градусов. Дно небольшой глубокой огнеупорной формы выстелить листом бумаги для выпечки или калькой. Перелить тесто в форму. Поставить в духовку и выпекать двадцать пять — тридцать минут до появления сахарной корочки.\n",
      "\n",
      "Готовый пирог вытащить из духовки, дать остыть и нарезать на квадратики острым ножом или ножом для пиццы — так кусочки получатся особенно ровными.\n",
      "\n",
      "Подавать брауни можно просто так, а можно посыпать сверху сахарной пудрой или разложить квадратики по тарелкам и украсить каждую порцию шариком ванильного мороженого.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "print(X_train[0])\n",
    "tokens = [tokenizer.tokenize_sentence(X_train[0])]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T08:26:16.430118800Z",
     "start_time": "2023-07-09T08:26:16.375800700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['шоколад', 'разлома', 'кусочк', 'вмест', 'сливочн', 'масл', 'растоп', 'водян', 'бан', 'перестав', 'врем', 'помешива', 'лопатк', 'деревя', 'ложк', 'получ', 'густ', 'шоколадн', 'соус', 'снят', 'водян', 'бан', 'остав', 'остыва', 'тем', 'времен', 'смеша', 'яйц', 'ста', 'грамм', 'коричнев', 'сахар', 'яйц', 'разб', 'отдельн', 'миск', 'взбит', 'постепен', 'добавл', 'сахар', 'взбива', 'помощ', 'миксер', 'вручн', '—', 'нрав', '—', 'меньш', 'двух', 'половиной-трех', 'минут', 'остр', 'нож', 'разделочн', 'доск', 'поруб', 'грецк', 'орех', 'предварительн', 'поджар', 'сух', 'сковород', 'появлен', 'аромат', 'эт', 'необязательн', 'опц', 'в', 'ост', 'растоплен', 'сливочн', 'масл', 'шоколад', 'аккуратн', 'добав', 'оста', 'сахар', 'зат', 'мук', 'измельчен', 'орех', 'перемеша', 'венчик', 'зат', 'влит', 'сахарно-яичн', 'сме', 'тщательн', 'смеша', 'шоколадн', 'масс', 'цвет', 'тест', 'долж', 'получ', 'равномерн', 'развод', 'разогрет', 'духовк', '200', 'градус', 'дно', 'небольш', 'глубок', 'огнеупорн', 'форм', 'выстел', 'лист', 'бумаг', 'выпечк', 'кальк', 'перел', 'тест', 'форм', 'постав', 'духовк', 'выпека', 'двадца', 'пят', '—', 'тридца', 'минут', 'появлен', 'сахарн', 'корочк', 'готов', 'пирог', 'вытащ', 'духовк', 'дат', 'ост', 'нареза', 'квадратик', 'остр', 'нож', 'нож', 'пицц', '—', 'кусочк', 'получат', 'особен', 'ровн', 'подава', 'браун', 'прост', 'посыпа', 'сверх', 'сахарн', 'пудр', 'разлож', 'квадратик', 'тарелк', 'украс', 'кажд', 'порц', 'шарик', 'ванильн', 'морожен']]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T21:48:01.859214Z",
     "start_time": "2023-07-08T21:48:01.834827600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n"
     ]
    }
   ],
   "source": [
    "vectorizer = BertVectorizer()\n",
    "vector = vectorizer.transform(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T22:08:50.250297600Z",
     "start_time": "2023-07-08T22:08:46.820521200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 5.57968393e-02,  6.69642329e-01, -1.00083530e-01,  3.22767824e-01,\n        5.94864130e-01, -2.67580420e-01,  1.91680398e-02, -4.65728849e-01,\n       -6.62021279e-01,  3.78983200e-01,  2.87605792e-01, -4.56742615e-01,\n       -1.90744549e-01, -1.88022584e-01, -1.61381841e-01,  6.41829222e-02,\n        5.47489047e-01, -3.07966113e-01,  2.97192991e-01,  4.78653342e-01,\n       -4.10718381e-01,  1.22976698e-01, -2.92649895e-01, -1.15826724e-05,\n        5.53830683e-01,  3.95425975e-01,  1.73098341e-01,  5.37239134e-01,\n        2.21128494e-01,  6.37348294e-01,  3.17309469e-01,  6.65508151e-01,\n        3.98610353e-01,  1.49739575e+00, -2.13135526e-01, -5.88761270e-01,\n       -9.27844048e-02, -1.42174855e-01, -1.19628347e-01,  1.65554717e-01,\n       -4.98822153e-01,  8.94496441e-01,  3.16652626e-01,  2.89316237e-01,\n        1.74797297e-01, -1.02147865e+00, -4.69532669e-01,  4.51981843e-01,\n       -3.93779039e-01,  5.54213107e-01,  3.86881083e-02, -1.36399639e+00,\n        7.02639446e-02, -2.53140241e-01, -1.40032083e-01,  1.23178899e-01,\n       -5.51933527e-01, -3.91054779e-01, -2.09562495e-01, -6.55146718e-01,\n        3.88298929e-01,  1.09572113e-02,  5.12866497e-01, -1.10277116e-01,\n        1.60375863e-01, -3.63840573e-02, -9.04694676e-01,  5.27209640e-02,\n       -1.18111096e-01,  5.75021744e-01,  1.78458691e-01,  6.33296251e-01,\n        6.32750273e-01,  1.93425447e-01,  5.15635133e-01,  1.88885379e+00,\n        4.39049676e-02,  2.60509521e-01, -6.95272386e-01, -5.82525373e-01,\n       -6.64182976e-02, -3.11613768e-01,  3.22509825e-01, -2.73488253e-01,\n       -4.71999466e-01, -1.37162626e-01, -2.67670065e-01, -8.56189609e-01,\n        8.46715085e-03, -1.28970599e+00, -1.14298666e+00,  1.17203116e-01,\n        1.01603962e-01,  6.53868079e-01, -1.95027605e-01,  1.61722255e+00,\n        8.24406028e-01, -1.62773341e-01,  5.38644791e-01, -2.46074870e-01,\n       -6.32048965e-01,  8.51811022e-02,  8.73020291e-02, -5.00255942e-01,\n        7.40913689e-01, -1.25907168e-01,  1.34427696e-01, -7.86134243e-01,\n       -7.30411410e-02,  1.86781156e+00, -8.28817263e-02,  3.47653717e-01,\n       -1.54566273e-01, -2.70210147e-01, -2.58486718e-01, -1.04539478e+00,\n        3.50159928e-02, -4.18415457e-01,  1.33263993e+00, -1.11913487e-01,\n       -1.67130113e-01,  1.90609112e-01, -2.93253958e-02, -4.94523346e-01,\n       -8.82613007e-03, -5.36541402e-01,  6.24938607e-01, -4.47976589e-01,\n       -2.92367674e-02,  3.32365274e-01, -4.34780508e-01, -9.15570140e-01,\n        4.22538429e-01,  9.55196396e-02, -2.28759363e-01, -1.02275395e+00,\n        7.41077542e-01,  2.47520134e-01, -7.64949694e-02, -1.29826498e+00,\n       -4.16633070e-01,  4.68205392e-01, -4.95997518e-01, -2.88736999e-01,\n       -1.06486298e-01,  4.45824802e-01, -7.99092412e-01, -4.89733629e-02,\n       -3.79711211e-01, -4.57579792e-01,  3.64633173e-01,  2.64340729e-01,\n        3.18191379e-01, -1.32990330e-02, -1.24980903e+00, -1.28551215e-01,\n        2.51444668e-01, -7.61736453e-01,  1.04953289e-01,  8.79809976e-01,\n        9.70368743e-01,  7.53626585e-01, -3.07617486e-01,  8.97434235e-01,\n        8.48178267e-01,  2.95970708e-01, -9.31165397e-01, -2.24607185e-01,\n        1.49034318e-02, -1.23738241e-03, -1.00053668e+00,  3.34106356e-01,\n       -2.10923165e-01,  3.59723061e-01,  8.31238389e-01, -1.69907749e-01,\n        8.03733021e-02,  2.17231721e-01,  1.43682659e+00, -1.98904440e-01,\n        1.71215042e-01,  1.18702412e+00, -1.95039958e-01,  4.60667074e-01,\n       -9.31278616e-02, -8.48145187e-01,  7.36380935e-01, -1.23600028e-01,\n       -2.68984467e-01,  4.33448404e-01,  4.62948650e-01,  1.55280039e-01,\n        8.43578339e-01,  2.34346718e-01,  4.82114136e-01,  5.13197243e-01,\n       -7.32878447e-01, -4.23222393e-01, -2.66118228e-01,  1.80450231e-01,\n        3.85659248e-01,  1.59253076e-01,  1.00065254e-01, -1.78415567e-01,\n        2.13378102e-01,  1.74413371e+00, -4.20456797e-01, -4.08497483e-01,\n       -1.63959071e-01,  8.31147850e-01, -3.49975020e-01,  4.31638718e-01,\n        3.38696688e-01, -9.22521949e-01, -2.95166075e-01,  8.93186554e-02,\n        1.23363674e+00, -2.50325292e-01,  2.73741186e-01, -3.90414745e-01,\n       -2.55408227e-01, -3.83298658e-03, -1.40353322e-01,  4.67629373e-01,\n       -2.26021975e-01, -2.98091531e-01, -5.02793372e-01,  7.20126271e-01,\n       -9.48245004e-02,  4.40930091e-02, -8.94585967e-01, -1.15436876e+00,\n        1.17328143e+00,  7.18482673e-01,  5.06698191e-02,  1.79180428e-01,\n        5.03825918e-02, -1.67645365e-01,  5.12498498e-01, -5.23006506e-02,\n       -5.15019298e-02,  6.74247921e-01, -6.91863894e-01,  8.30099225e-01,\n        4.73733097e-01,  4.75899130e-01, -6.36815667e-01, -2.45569140e-01,\n        1.51947021e+00, -9.93364453e-01, -3.98788154e-01,  1.02211368e+00,\n        3.53124797e-01, -2.43976906e-01, -1.62420106e+00, -6.80575550e-01,\n        1.11111319e+00,  6.67882562e-01,  2.41864529e-02, -4.50014398e-02,\n       -5.96813075e-02, -4.11220551e-01, -2.54596949e-01,  9.58699211e-02,\n       -3.76461297e-02,  9.57432166e-02,  1.88020077e-02,  8.03004444e-01,\n       -4.37995106e-01, -8.84731263e-02, -4.69998986e-01, -1.65180266e-02,\n        9.55080390e-01,  1.03284621e+00, -5.30217946e-01, -3.47957581e-01,\n       -1.45067424e-01, -6.99467897e-01, -1.15437508e-01,  6.89565301e-01,\n        2.35751241e-01,  8.65153193e-01,  2.89249599e-01, -1.58436370e+00,\n       -8.36628079e-01,  6.10663295e-01, -2.13425756e-01,  1.41534582e-01,\n       -8.54415119e-01, -8.14887702e-01,  3.97319734e-01,  1.18703008e-01,\n        2.91968524e-01, -1.02042627e+00,  3.24569166e-01, -6.32700205e-01,\n       -8.55420768e-01, -4.10784006e-01,  5.79565525e-01, -1.10085741e-01,\n        2.74581581e-01,  4.45383310e-01, -8.52126241e-01, -1.71970487e-01,\n       -8.79080594e-01, -9.29806232e-02,  5.48884988e-01,  2.77742684e-01,\n        4.56002414e-01, -6.48410976e-01, -1.12840354e+00,  7.27607429e-01,\n       -7.81079650e-01, -1.10917553e-01, -4.19481993e-01,  1.13167000e+00,\n        1.94759533e-01,  5.10478079e-01,  4.01750654e-01, -5.20843625e-01,\n       -1.05010068e+00, -4.06244099e-01,  2.80832589e-01, -2.50687543e-02,\n        1.63453773e-01,  6.20420754e-01, -1.02642322e+00, -1.09061027e+00,\n       -4.08101380e-01, -7.27516532e-01, -3.92834306e-01, -6.10114753e-01,\n       -1.61989689e-01, -6.71014488e-01, -3.06245446e-01, -9.87662673e-02,\n        8.05085748e-02, -9.38822925e-02,  4.61349897e-02, -3.92833471e-01,\n       -3.25735025e-02,  6.81079805e-01,  7.01654851e-02, -7.36474618e-02,\n       -3.51543576e-01,  9.34565425e-01,  9.26255524e-01,  1.39135623e+00,\n       -8.77085030e-01,  1.68076396e-01,  1.38633370e+00,  2.99831927e-02,\n        4.49180692e-01, -5.24089456e-01,  4.53872740e-01, -5.57154655e-01,\n       -5.88691458e-02,  8.37933660e-01,  4.04249579e-01,  1.26041090e+00,\n       -5.59284464e-02,  5.30400395e-01, -4.37137127e-01, -1.79310128e-01,\n       -2.95204580e-01,  2.36816436e-01,  3.58410716e-01, -6.28639162e-02,\n       -3.10461879e-01,  5.96495271e-01,  8.35785735e-03,  1.20161247e+00,\n       -5.26487231e-01, -1.00769877e+00, -1.02696073e+00,  4.55432162e-02,\n        4.40620512e-01,  6.81807876e-01,  5.80143705e-02,  1.76688097e-02,\n        3.67736280e-01,  3.83393526e-01,  8.90884995e-01,  4.46003899e-02,\n        8.24710488e-01, -5.46093822e-01, -4.19553429e-01, -2.08577442e+00,\n       -2.54452050e-01, -7.37281516e-02,  1.08769453e+00,  1.10879861e-01,\n       -5.14658511e-01, -4.47183281e-01, -8.46049786e-01,  4.98921573e-01,\n       -8.85226503e-02,  6.95346951e-01, -1.10048223e+00, -2.56530106e-01,\n       -9.34345245e-01,  3.94764572e-01,  5.55283070e-01,  1.46798682e+00,\n       -2.68937558e-01,  5.10959446e-01,  9.89634216e-01,  2.32621193e-01,\n        4.54105198e-01,  7.37185419e-01,  3.77466440e-01,  4.58523750e-01,\n        2.25773007e-01,  8.29267800e-01, -1.26860753e-01,  8.31509590e-01,\n       -1.12410760e+00,  1.89374074e-01,  2.75094688e-01,  1.31638229e+00,\n       -3.67626995e-02, -7.96410680e-01,  5.00962615e-01, -2.18821478e+00,\n        8.77272114e-02,  5.47484696e-01,  3.22159469e-01, -1.95921093e-01,\n        5.16190052e-01,  3.22066471e-02, -7.69701183e-01,  7.16725349e-01,\n       -4.50158834e-01, -1.29839182e-01, -8.63899142e-02, -1.61452323e-01,\n       -3.39396685e-01,  2.74697721e-01,  5.48663363e-02, -5.76643229e-01,\n        5.97454190e-01, -6.63569152e-01,  2.09820941e-01, -6.78651631e-01,\n        1.09368049e-01, -2.05369925e+00, -2.36368179e-01, -1.19575262e+00,\n       -2.26312682e-01,  2.58577559e-02,  4.10055459e-01, -1.27137661e+00,\n       -1.31000862e-01, -6.91028416e-01,  2.70958483e-01, -6.42497659e-01,\n       -3.31080288e-01,  4.71364647e-01, -3.56775433e-01,  2.13100649e-02,\n       -7.74221301e-01, -1.32241952e+00, -2.64415201e-02, -2.52765089e-01,\n        9.57930684e-02, -2.25542784e-01, -4.41496074e-02, -4.01043415e-01,\n        2.88048685e-01,  2.61111808e+00, -8.74428898e-02,  6.58044219e-01,\n       -3.43357384e-01,  5.49102724e-01, -5.74292958e-01,  8.54265034e-01,\n       -5.89788258e-01, -5.50325751e-01, -6.21343195e-01, -4.52698410e-01,\n       -4.46906477e-01,  7.81601787e-01,  3.57662737e-01, -1.21722054e+00,\n       -3.42954218e-01,  6.50479853e-01,  1.36186391e-01, -5.62394559e-01,\n       -1.48707539e-01,  5.13398528e-01, -6.50861263e-01, -9.45445061e-01,\n       -5.18526673e-01, -5.66694140e-01,  6.90148056e-01, -6.24931812e-01,\n        5.31259298e-01,  3.98196056e-02,  1.09618032e+00, -5.82006752e-01,\n        2.82413930e-01,  6.34472787e-01,  3.77226651e-01,  4.81875420e-01,\n       -2.44458348e-01, -1.50111303e-01,  2.39769191e-01,  3.65230143e-01,\n       -4.85736758e-01,  6.31458879e-01, -2.61015743e-01, -6.92159057e-01,\n       -4.54541862e-01,  3.34264338e-01,  7.01233149e-01, -3.53674442e-01,\n       -4.47026551e-01, -1.22589612e+00,  6.61280870e-01,  3.65632065e-02,\n       -7.10099265e-02, -2.73701698e-01,  2.70759821e-01,  4.07743514e-01,\n       -1.39046296e-01, -5.09211659e-01, -3.76537800e-01, -2.56919801e-01,\n       -1.50276214e-01,  2.05184311e-01, -1.17599986e-01,  3.80117834e-01,\n       -2.00190216e-01,  2.45464325e-01,  3.07911098e-01,  1.19690824e+00,\n       -8.67513061e-01, -8.35475028e-02, -1.02611625e+00, -8.42169672e-02,\n        3.73890013e-01,  1.34869097e-02, -6.39557004e-01,  8.25716406e-02,\n       -1.32048696e-01,  9.53946114e-02, -2.90616691e-01, -6.50087744e-03,\n       -1.09491730e+00, -7.51855910e-01,  1.81946419e-02, -3.22711170e-01,\n        3.09019506e-01, -3.44098985e-01, -1.26693398e-01,  8.90123725e-01,\n        4.88410652e-01,  1.73639618e-02, -3.70463938e-01, -1.13125004e-01,\n       -6.90839469e-01,  1.39546919e+00, -3.08975633e-02, -6.67105675e-01,\n        4.86948907e-01, -2.39947125e-01, -3.30717027e-01,  2.07977891e-01,\n       -5.88069797e-01,  8.57419491e-01,  3.61368924e-01,  1.17464590e+00,\n       -9.22663212e-01, -4.12658453e-01,  3.27423781e-01, -4.28551197e-01,\n       -1.05256045e+00, -5.95721185e-01, -7.53430605e-01, -8.49004269e-01,\n        5.15682280e-01, -3.63147967e-02,  4.77869034e-01,  2.37254307e-01,\n       -4.16616350e-01,  4.68236268e-01,  1.73696741e-01, -5.42497039e-01,\n       -4.71803010e-01, -4.02330965e-01, -1.12345469e+00, -2.06446365e-01,\n       -1.11877069e-01,  6.55519485e-01,  4.81928438e-01, -8.51276517e-01,\n        8.18579853e-01,  2.68060565e-01,  5.98040044e-01, -2.60247648e-01,\n        1.50438774e+00,  2.34642506e-01,  4.99538034e-01,  4.16945398e-01,\n       -1.58971977e+00,  4.56000417e-01, -4.67688411e-01,  1.89082310e-01,\n       -1.49817586e+00,  6.97560906e-01,  6.32515728e-01, -2.94863880e-01,\n       -6.88367903e-01,  2.56305009e-01, -4.05605063e-02, -4.88422543e-01,\n       -1.04024971e+00, -3.38559717e-01,  4.05033171e-01,  3.38512748e-01,\n       -2.39203960e-01, -4.05243039e-03, -6.02363423e-02, -2.25852653e-01,\n        1.08910706e-02, -8.78783405e-01, -2.76567727e-01, -5.47735095e-01,\n        1.33518308e-01, -7.86039472e-01,  3.25040728e-01, -1.97101802e-01,\n        4.23514307e-01,  1.45907374e-02, -3.93249840e-01,  1.28016853e+00,\n       -5.32691658e-01, -3.48424137e-01,  2.79402912e-01, -3.88003469e-01,\n       -1.42615348e-01,  5.46603918e-01, -9.36795592e-01, -8.15582871e-01,\n        1.03683841e+00,  5.55794597e-01, -1.99821305e+00, -1.58922508e-01,\n        1.19267724e-01, -5.92012227e-01, -1.21659541e+00, -1.75999188e+00,\n        2.39097521e-01,  5.70733845e-01,  5.58164716e-01,  9.80592132e-01,\n       -5.16643167e-01, -5.95503271e-01,  5.06367683e-01,  6.14612818e-01,\n        3.74274015e-01,  1.00209522e+00, -2.95715153e-01, -8.98190498e-01,\n       -1.40743756e+00,  8.57452154e-02,  8.37319568e-02,  2.67240051e-02,\n        6.11899018e-01,  4.27762508e-01, -3.63843143e-01, -8.07855487e-01,\n        8.54101658e-01,  1.38763404e+00,  1.04845655e+00, -5.39567411e-01,\n       -1.09232269e-01,  8.16396236e-01, -6.34770632e-01,  1.05766273e+00,\n       -1.20899904e+00,  4.96947378e-01, -2.91841514e-02, -6.97103322e-01,\n       -5.42764254e-02,  3.35661680e-01,  9.52798724e-01, -1.12505364e+00,\n       -1.45404696e-01, -4.31630045e-01,  7.61718750e-01,  3.74700159e-01,\n       -6.31155789e-01, -4.72015530e-01,  2.23900512e-01,  1.45877647e+00,\n        5.36833704e-01,  1.93867773e-01, -2.13900600e-02, -6.39161348e-01,\n        3.12529147e-01, -1.02085662e+00,  5.03935456e-01, -4.19332802e-01,\n       -1.75002396e-01, -5.08574009e-01,  3.24800424e-02, -4.62836921e-01,\n        7.25289941e-01,  6.27792835e-01,  1.16395272e-01,  6.02867186e-01,\n       -1.51363283e-01,  2.52244234e-01, -2.22700447e-01,  7.50373304e-01,\n       -3.94347787e-01, -7.64102399e-01, -5.08033633e-01,  6.85299456e-01,\n        7.90686011e-01,  1.28902912e-01, -4.95732486e-01,  1.67071428e-02,\n       -4.86478865e-01, -1.64773047e-01,  1.23398259e-01,  6.29223943e-01,\n       -1.66518897e-01, -9.06219184e-01,  2.92000324e-01, -3.20117831e+00,\n       -6.13340251e-02,  4.25026566e-02, -8.99688900e-02, -7.58165658e-01,\n        1.26220554e-01,  1.65497154e-01, -1.08405471e+00, -2.52324402e-01,\n        1.95531160e-01, -5.25351346e-01,  4.83604819e-01,  6.11586682e-02,\n        3.17185014e-01, -6.53639197e-01,  6.89635992e-01, -2.83614933e-01,\n        2.89076567e-01, -1.82216895e+00,  7.93654740e-01,  9.07449603e-01,\n       -1.33199275e-01,  2.61363208e-01, -3.31709474e-01,  2.84995019e-01,\n       -3.14105928e-01, -6.44052774e-02,  3.09720486e-01,  2.56604731e-01,\n       -1.04532576e+00, -2.97465146e-01,  4.04178321e-01,  3.00818115e-01,\n        1.01724052e+00,  1.07525587e+00,  1.57456458e-01,  2.52969742e-01],\n      dtype=float32)"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T22:05:55.967954900Z",
     "start_time": "2023-07-08T22:05:55.915337Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.05955216769890424"
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = Pipeline([\n",
    "    (\"vectorizer\", BertVectorizer2()),\n",
    "    (\"normalizer\", Normalizer()),\n",
    "    (\"model\",  KNeighborsClassifier())\n",
    "]\n",
    ")\n",
    "knn.fit(X_train, y_train)\n",
    "accuracy_score(y_test, knn.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T08:50:43.725616100Z",
     "start_time": "2023-07-09T08:45:42.404892400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zxc ghoul\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.16055264411624584"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(tokenizer=lambda x: tokenize_sentence(x, remove_stop_words=True), max_features=768)),\n",
    "    (\"normalizer\", Normalizer()),\n",
    "    (\"model\",  KNeighborsClassifier())\n",
    "]\n",
    ")\n",
    "knn.fit(X_train, y_train)\n",
    "accuracy_score(y_test, knn.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T08:57:16.984643200Z",
     "start_time": "2023-07-09T08:56:35.565485300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn = Pipeline([\n",
    "    (\"vectorizer\", BertVectorizer()),\n",
    "    (\"normalizer\", Normalizer()),\n",
    "    (\"model\",  KNeighborsClassifier())\n",
    "]\n",
    ")\n",
    "knn.fit(X_train, y_train)\n",
    "accuracy_score(y_test, knn.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03620771796093378\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[142], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(accuracy)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# creating a confusion matrix\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m knn_predictions \u001B[38;5;241m=\u001B[39m \u001B[43mknn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py:507\u001B[0m, in \u001B[0;36mPipeline.predict\u001B[1;34m(self, X, **predict_params)\u001B[0m\n\u001B[0;32m    505\u001B[0m Xt \u001B[38;5;241m=\u001B[39m X\n\u001B[0;32m    506\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, name, transform \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter(with_final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m--> 507\u001B[0m     Xt \u001B[38;5;241m=\u001B[39m \u001B[43mtransform\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    508\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mpredict(Xt, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpredict_params)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\_set_output.py:140\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 140\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    142\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[0;32m    143\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    144\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    145\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[0;32m    146\u001B[0m         )\n",
      "Cell \u001B[1;32mIn[140], line 23\u001B[0m, in \u001B[0;36mBertVectorizer.transform\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m     20\u001B[0m input_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(input_ids)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 23\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     26\u001B[0m text_vector \u001B[38;5;241m=\u001B[39m embeddings\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mmean(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1020\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1011\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m   1013\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[0;32m   1014\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   1015\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1018\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[0;32m   1019\u001B[0m )\n\u001B[1;32m-> 1020\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1021\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1022\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1023\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1024\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1025\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1026\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1027\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1028\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1029\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1030\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1031\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1032\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1033\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001B[0m, in \u001B[0;36mBertEncoder.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    601\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[0;32m    602\u001B[0m         create_custom_forward(layer_module),\n\u001B[0;32m    603\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    607\u001B[0m         encoder_attention_mask,\n\u001B[0;32m    608\u001B[0m     )\n\u001B[0;32m    609\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 610\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    611\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    612\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    613\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    614\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    615\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    616\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    617\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    618\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    620\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    621\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\bert\\modeling_bert.py:495\u001B[0m, in \u001B[0;36mBertLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    483\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    484\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    485\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    492\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[0;32m    494\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 495\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    502\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    504\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "accuracy = knn.score(X_test, y_test)\n",
    "print(accuracy)\n",
    "\n",
    "# creating a confusion matrix\n",
    "knn_predictions = knn.predict(X_test) #0.16055264411624584\n",
    "#cm = confusion_matrix(y_test, knn_predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T21:45:33.000421400Z",
     "start_time": "2023-07-08T21:37:03.286341900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "class FaissKNeighbors:\n",
    "    def __init__(self, k=5):\n",
    "        self.index = None\n",
    "        self.y = None\n",
    "        self.k = k\n",
    "        self.vectorizer = TfidfVectorizer(tokenizer=lambda x: tokenize_sentence(x, remove_stop_words=True))\n",
    "        self.normalizer = Normalizer()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = self.vectorizer.fit_transform(X)\n",
    "        X = self.normalizer.fit_transform(X)\n",
    "        X = X.toarray().astype(np.float32)\n",
    "\n",
    "        dimension = X.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.y = y\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i].astype(np.float32)\n",
    "            self.index.add(x)\n",
    "\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.vectorizer.transform(X)\n",
    "        X = self.normalizer.transform(X)\n",
    "        X = X.toarray().astype(np.float32)  # Преобразование разреженной матрицы в массив плотных векторов\n",
    "        distances, indices = self.index.search(X, k=self.k)\n",
    "        votes = self.y[indices]\n",
    "        predictions = np.array([np.argmax(np.bincount(x)) for x in votes])\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T08:16:51.306791400Z",
     "start_time": "2023-07-09T08:16:51.297270Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zxc ghoul\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[170], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m knn_faiss \u001B[38;5;241m=\u001B[39m Pipeline([\n\u001B[0;32m      2\u001B[0m     (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, FaissKNeighbors())\n\u001B[0;32m      3\u001B[0m ])\n\u001B[1;32m----> 5\u001B[0m \u001B[43mknn_faiss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:1151\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1144\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1147\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1148\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1149\u001B[0m     )\n\u001B[0;32m   1150\u001B[0m ):\n\u001B[1;32m-> 1151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py:420\u001B[0m, in \u001B[0;36mPipeline.fit\u001B[1;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[0;32m    418\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_final_estimator \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpassthrough\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    419\u001B[0m         fit_params_last_step \u001B[38;5;241m=\u001B[39m fit_params_steps[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m0\u001B[39m]]\n\u001B[1;32m--> 420\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_final_estimator\u001B[38;5;241m.\u001B[39mfit(Xt, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params_last_step)\n\u001B[0;32m    422\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "Cell \u001B[1;32mIn[169], line 20\u001B[0m, in \u001B[0;36mFaissKNeighbors.fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n\u001B[0;32m     19\u001B[0m     x \u001B[38;5;241m=\u001B[39m X[i]\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m---> 20\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my \u001B[38;5;241m=\u001B[39m y\n",
      "File \u001B[1;32mC:\\ProgramData\\miniconda3\\lib\\site-packages\\faiss\\class_wrappers.py:227\u001B[0m, in \u001B[0;36mhandle_Index.<locals>.replacement_add\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreplacement_add\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m    215\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Adds vectors to the index.\u001B[39;00m\n\u001B[0;32m    216\u001B[0m \u001B[38;5;124;03m    The index must be trained before vectors can be added to it.\u001B[39;00m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;124;03m    The vectors are implicitly numbered in sequence. When `n` vectors are\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    224\u001B[0m \u001B[38;5;124;03m        `dtype` must be float32.\u001B[39;00m\n\u001B[0;32m    225\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 227\u001B[0m     n, d \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m    228\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m d \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md\n\u001B[0;32m    229\u001B[0m     x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mascontiguousarray(x, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "knn_faiss = Pipeline([\n",
    "    (\"model\", FaissKNeighbors())\n",
    "])\n",
    "\n",
    "knn_faiss.fit(X_train, y_train)\n",
    "#accuracy_score(y_test, knn_faiss.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T08:17:19.684200200Z",
     "start_time": "2023-07-09T08:16:55.177805500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([(4528, 3823, 4125, 2848, 4070), (1022, 2780, 2971, 4207, 3028),\\n         (2821, 15, 2662, 1171, 4036), (1771, 1158, 3840, 2365, 1461),\\n       (2030, 1937, 3960, 3235, 1880),  (3940, 2678, 462, 4371, 4356),\\n       (1675, 2913, 3365, 1759, 4545), (3702, 4029, 2714, 4108, 3992),\\n         (1776, 775, 2174, 1017, 652),   (1881, 3568, 2155, 71, 2140),\\n       ...\\n        (4793, 3721, 2729, 2075, 651),  (2246, 3161, 261, 4818, 3039),\\n         (2259, 565, 266, 3447, 1438),  (4004, 207, 3885, 2165, 1725),\\n       (2064, 2550, 2450, 3502, 4522),   (51, 2954, 4887, 2997, 4802),\\n         (3287, 2024, 1369, 3196, 38),    (2899, 3047, 242, 50, 2561),\\n         (918, 4021, 3874, 3889, 155),  (2999, 3478, 4396, 147, 4458)],\\n      dtype='object', length=2099)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[43], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mknn_faiss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\pipeline.py:508\u001B[0m, in \u001B[0;36mPipeline.predict\u001B[1;34m(self, X, **predict_params)\u001B[0m\n\u001B[0;32m    506\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, name, transform \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter(with_final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    507\u001B[0m     Xt \u001B[38;5;241m=\u001B[39m transform\u001B[38;5;241m.\u001B[39mtransform(Xt)\n\u001B[1;32m--> 508\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mpredict(Xt, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpredict_params)\n",
      "Cell \u001B[1;32mIn[41], line 27\u001B[0m, in \u001B[0;36mFaissKNeighbors.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m     25\u001B[0m X \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mtoarray()\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32)  \u001B[38;5;66;03m# Преобразование разреженной матрицы в массив плотных векторов\u001B[39;00m\n\u001B[0;32m     26\u001B[0m distances, indices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39msearch(X, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk)\n\u001B[1;32m---> 27\u001B[0m votes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindices\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     28\u001B[0m predictions \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([np\u001B[38;5;241m.\u001B[39margmax(np\u001B[38;5;241m.\u001B[39mbincount(x)) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m votes])\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m predictions\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\frame.py:3767\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3765\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[0;32m   3766\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[1;32m-> 3767\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcolumns\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m   3769\u001B[0m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[0;32m   3770\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:5877\u001B[0m, in \u001B[0;36mIndex._get_indexer_strict\u001B[1;34m(self, key, axis_name)\u001B[0m\n\u001B[0;32m   5874\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   5875\u001B[0m     keyarr, indexer, new_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_non_unique(keyarr)\n\u001B[1;32m-> 5877\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   5879\u001B[0m keyarr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(indexer)\n\u001B[0;32m   5880\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[0;32m   5881\u001B[0m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:5938\u001B[0m, in \u001B[0;36mIndex._raise_if_missing\u001B[1;34m(self, key, indexer, axis_name)\u001B[0m\n\u001B[0;32m   5936\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m use_interval_msg:\n\u001B[0;32m   5937\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[1;32m-> 5938\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   5940\u001B[0m not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39munique())\n\u001B[0;32m   5941\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"None of [Index([(4528, 3823, 4125, 2848, 4070), (1022, 2780, 2971, 4207, 3028),\\n         (2821, 15, 2662, 1171, 4036), (1771, 1158, 3840, 2365, 1461),\\n       (2030, 1937, 3960, 3235, 1880),  (3940, 2678, 462, 4371, 4356),\\n       (1675, 2913, 3365, 1759, 4545), (3702, 4029, 2714, 4108, 3992),\\n         (1776, 775, 2174, 1017, 652),   (1881, 3568, 2155, 71, 2140),\\n       ...\\n        (4793, 3721, 2729, 2075, 651),  (2246, 3161, 261, 4818, 3039),\\n         (2259, 565, 266, 3447, 1438),  (4004, 207, 3885, 2165, 1725),\\n       (2064, 2550, 2450, 3502, 4522),   (51, 2954, 4887, 2997, 4802),\\n         (3287, 2024, 1369, 3196, 38),    (2899, 3047, 242, 50, 2561),\\n         (918, 4021, 3874, 3889, 155),  (2999, 3478, 4396, 147, 4458)],\\n      dtype='object', length=2099)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "knn_faiss.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T19:02:42.230117800Z",
     "start_time": "2023-07-08T19:02:24.240174800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
