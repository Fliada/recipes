{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-07T20:58:06.952825300Z",
     "start_time": "2023-07-07T20:58:06.933827100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\zxc\n",
      "[nltk_data]     ghoul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve, accuracy_score,hamming_loss\n",
    "import numpy as np\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import sklearn.utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '4', '5', '7', '8', '14', '16', '17', '20', '25', '27', '30', '33', '37', '43', '51', '62', '67', '68', '71', '90', '99', '103', '105', '139', '140', '150', '158', '257']\n"
     ]
    }
   ],
   "source": [
    "db_name = 'recipe.db'\n",
    "conn = sqlite3.connect(db_name)\n",
    "\n",
    "table_name = 'recipe'\n",
    "\n",
    "#забираем доступные категории\n",
    "categories_sql = f\"select c.id, c.name_cat from category c \" \\\n",
    "                 f\"join recipe_categories rc on \" \\\n",
    "                 f\"c.id = rc.cat_id \" \\\n",
    "                 f\"GROUP by c.id \" \\\n",
    "                 f\"HAVING count(*) > 100 \" \\\n",
    "                 f\"ORDER BY c.id\"\n",
    "\n",
    "loaded_categories = pd.read_sql(categories_sql, conn)\n",
    "\n",
    "ids_cat = []\n",
    "for i in range(len(loaded_categories)):\n",
    "    ids_cat.append(str(loaded_categories.id[i]))\n",
    "\n",
    "print(ids_cat)\n",
    "\n",
    "#забираем способ приготовления рецепта с категориями из доступных с id категорий\n",
    "sql = f\"SELECT r.manual, (SELECT group_concat(rc.cat_id , ', ') \" \\\n",
    "      f\"from recipe_categories rc \" \\\n",
    "      f\"WHERE rc.recipe_id = r.id AND \" \\\n",
    "      f\"rc.cat_id in ({', '.join(ids_cat)})) \" \\\n",
    "      f\"AS categories from recipe r\"\n",
    "\n",
    "loaded_data = pd.read_sql(sql, conn)\n",
    "\n",
    "#pandas data frame\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T20:19:31.699157800Z",
     "start_time": "2023-07-07T20:19:20.074848500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "for i in range(len(ids_cat)):\n",
    "    val = []\n",
    "    for j in range(len(loaded_data)):\n",
    "        spl = str(loaded_data.categories[j]).split(', ')\n",
    "        #print(spl)\n",
    "        #print(int(str(ids_cat[i]) in spl))\n",
    "        val.append(int(str(ids_cat[i]) in spl))\n",
    "    loaded_data.insert(loc=len(loaded_data.columns) , column=ids_cat[i], value=val)\n",
    "#print(val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T20:19:33.406822400Z",
     "start_time": "2023-07-07T20:19:31.698138300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#делим dataset на тренировочный и тестовый\n",
    "X_train,X_test,y_train,y_test = train_test_split(loaded_data[\"manual\"], loaded_data[ids_cat],test_size=0.3,random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T20:19:33.460117100Z",
     "start_time": "2023-07-07T20:19:33.411950800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer(language=\"russian\")\n",
    "russian_stop_words = stopwords.words(\"russian\")\n",
    "\n",
    "def tokenize_sentence(sentence: str, remove_stop_words: bool = True):\n",
    "    tokens = word_tokenize(sentence, language=\"russian\")\n",
    "    tokens = [i for i in tokens if i not in string.punctuation]\n",
    "    if remove_stop_words:\n",
    "        tokens = [i for i in tokens if i not in russian_stop_words]\n",
    "    tokens = [snowball.stem(i) for i in tokens]\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T20:19:33.474323900Z",
     "start_time": "2023-07-07T20:19:33.461365100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zxc ghoul\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.16055264411624584"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = Pipeline([\n",
    "    (\"vectorizer\",  TfidfVectorizer(tokenizer=lambda x: tokenize_sentence(x, remove_stop_words=True), max_features=768)),\n",
    "    (\"normalizer\", Normalizer()),\n",
    "    (\"model\",  KNeighborsClassifier())\n",
    "]\n",
    ")\n",
    "knn.fit(X_train, y_train)\n",
    "accuracy_score(y_test, knn.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T20:20:08.771324300Z",
     "start_time": "2023-07-07T20:19:33.479077700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16055264411624584\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multilabel-indicator is not supported",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# creating a confusion matrix\u001B[39;00m\n\u001B[0;32m      5\u001B[0m knn_predictions \u001B[38;5;241m=\u001B[39m knn\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[1;32m----> 6\u001B[0m cm \u001B[38;5;241m=\u001B[39m \u001B[43mconfusion_matrix\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mknn_predictions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:319\u001B[0m, in \u001B[0;36mconfusion_matrix\u001B[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001B[0m\n\u001B[0;32m    317\u001B[0m y_type, y_true, y_pred \u001B[38;5;241m=\u001B[39m _check_targets(y_true, y_pred)\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulticlass\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 319\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m is not supported\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m y_type)\n\u001B[0;32m    321\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    322\u001B[0m     labels \u001B[38;5;241m=\u001B[39m unique_labels(y_true, y_pred)\n",
      "\u001B[1;31mValueError\u001B[0m: multilabel-indicator is not supported"
     ]
    }
   ],
   "source": [
    "accuracy = knn.score(X_test, y_test)\n",
    "print(accuracy)\n",
    "\n",
    "# creating a confusion matrix\n",
    "knn_predictions = knn.predict(X_test)\n",
    "#cm = confusion_matrix(y_test, knn_predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T20:20:39.288165800Z",
     "start_time": "2023-07-07T20:20:08.835344600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class FaissKNeighbors:\n",
    "    def __init__(self, k=5):\n",
    "        self.index = None\n",
    "        self.y = None\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.index = faiss.IndexFlatL2(X.shape[1])\n",
    "        self.index.add(X.astype(np.float32))\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        distances, indices = self.index.search(X.astype(np.float32), k=self.k)\n",
    "        votes = self.y[indices]\n",
    "        predictions = np.array([np.argmax(np.bincount(x)) for x in votes])\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T20:58:19.980986800Z",
     "start_time": "2023-07-07T20:58:19.960206600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zxc ghoul\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'faiss' has no attribute 'IndexFlatL2'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 8\u001B[0m\n\u001B[0;32m      1\u001B[0m knn_faisse \u001B[38;5;241m=\u001B[39m Pipeline([\n\u001B[0;32m      2\u001B[0m     (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvectorizer\u001B[39m\u001B[38;5;124m\"\u001B[39m,  TfidfVectorizer(tokenizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: tokenize_sentence(x, remove_stop_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m), max_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m768\u001B[39m)),\n\u001B[0;32m      3\u001B[0m     (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnormalizer\u001B[39m\u001B[38;5;124m\"\u001B[39m, Normalizer()),\n\u001B[0;32m      4\u001B[0m     (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m,  FaissKNeighbors())\n\u001B[0;32m      5\u001B[0m ]\n\u001B[0;32m      6\u001B[0m )\n\u001B[1;32m----> 8\u001B[0m \u001B[43mknn_faisse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m accuracy_score(y_test, knn\u001B[38;5;241m.\u001B[39mpredict(X_test))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\pipeline.py:405\u001B[0m, in \u001B[0;36mPipeline.fit\u001B[1;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[0;32m    403\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_final_estimator \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpassthrough\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    404\u001B[0m         fit_params_last_step \u001B[38;5;241m=\u001B[39m fit_params_steps[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m0\u001B[39m]]\n\u001B[1;32m--> 405\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_final_estimator\u001B[38;5;241m.\u001B[39mfit(Xt, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params_last_step)\n\u001B[0;32m    407\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "Cell \u001B[1;32mIn[16], line 8\u001B[0m, in \u001B[0;36mFaissKNeighbors.fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y):\n\u001B[1;32m----> 8\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex \u001B[38;5;241m=\u001B[39m \u001B[43mfaiss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mIndexFlatL2\u001B[49m(X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39madd(X\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32))\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my \u001B[38;5;241m=\u001B[39m y\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'faiss' has no attribute 'IndexFlatL2'"
     ]
    }
   ],
   "source": [
    "knn_faisse = Pipeline([\n",
    "    (\"vectorizer\",  TfidfVectorizer(tokenizer=lambda x: tokenize_sentence(x, remove_stop_words=True), max_features=768)),\n",
    "    (\"normalizer\", Normalizer()),\n",
    "    (\"model\",  FaissKNeighbors())\n",
    "]\n",
    ")\n",
    "\n",
    "knn_faisse.fit(X_train, y_train)\n",
    "accuracy_score(y_test, knn.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T20:58:39.602516500Z",
     "start_time": "2023-07-07T20:58:21.880026300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
