{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-06T21:23:18.491179700Z",
     "start_time": "2023-07-06T21:21:48.309751900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\zxc\n",
      "[nltk_data]     ghoul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve, accuracy_score,hamming_loss\n",
    "import numpy as np\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import sklearn.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "db_name = 'recipe.db'\n",
    "conn = sqlite3.connect(db_name)\n",
    "\n",
    "table_name = 'recipe'\n",
    "\n",
    "#забираем доступные категории\n",
    "categories_sql = f\"select c.id from category c \" \\\n",
    "                 f\"join recipe_categories rc on \" \\\n",
    "                 f\"c.id = rc.cat_id \" \\\n",
    "                 f\"GROUP by c.id \" \\\n",
    "                 f\"HAVING count(*) > 100 \" \\\n",
    "                 f\"ORDER BY c.id\"\n",
    "\n",
    "#забираем способ приготовления рецепта с категориями из доступных с id категорий\n",
    "sql = f\"SELECT r.manual, (SELECT group_concat(rc.cat_id , ', ') \" \\\n",
    "      f\"from recipe_categories rc \" \\\n",
    "      f\"WHERE rc.recipe_id = r.id AND \" \\\n",
    "      f\"rc.cat_id in ({categories_sql})\" \\\n",
    "      f\") AS categories from recipe r\"\n",
    "\n",
    "loaded_data = pd.read_sql(sql, conn)\n",
    "loaded_categories = pd.read_sql(categories_sql, conn)\n",
    "#pandas data frame\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T21:23:27.941178400Z",
     "start_time": "2023-07-06T21:23:18.500189300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '4', '5', '7', '8', '14', '16', '17', '20', '25', '27', '30', '33', '37', '43', '51', '62', '67', '68', '71', '90', '99', '103', '105', '139', '140', '150', '158', '257']\n"
     ]
    }
   ],
   "source": [
    "#print(loaded_categories.id)\n",
    "\n",
    "ids_cat = []\n",
    "for i in range(len(loaded_categories)):\n",
    "    ids_cat.append(str(loaded_categories.id[i]))\n",
    "\n",
    "print(ids_cat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T21:23:28.019110600Z",
     "start_time": "2023-07-06T21:23:27.941178400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "for i in range(len(ids_cat)):\n",
    "    val = []\n",
    "    for j in range(len(loaded_data)):\n",
    "        spl = str(loaded_data.categories[j]).split(', ')\n",
    "        #print(spl)\n",
    "        #print(int(str(ids_cat[i]) in spl))\n",
    "        val.append(int(str(ids_cat[i]) in spl))\n",
    "    loaded_data.insert(loc=len(loaded_data.columns) , column=ids_cat[i], value=val)\n",
    "#print(val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T21:23:29.518244Z",
     "start_time": "2023-07-06T21:23:27.969380300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#делим dataset на тренировочный и тестовый\n",
    "x_train,x_test,y_train,y_test = train_test_split(loaded_data[\"manual\"], loaded_data[ids_cat],test_size=0.3,random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T21:23:29.573410Z",
     "start_time": "2023-07-06T21:23:29.528258Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer(language=\"russian\")\n",
    "russian_stop_words = stopwords.words(\"russian\")\n",
    "\n",
    "def tokenize_sentence(sentence: str, remove_stop_words: bool = True):\n",
    "    tokens = word_tokenize(sentence, language=\"russian\")\n",
    "    tokens = [i for i in tokens if i not in string.punctuation]\n",
    "    if remove_stop_words:\n",
    "        tokens = [i for i in tokens if i not in russian_stop_words]\n",
    "    tokens = [snowball.stem(i) for i in tokens]\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T21:23:29.605206300Z",
     "start_time": "2023-07-06T21:23:29.573410Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Положите весь творог в кастрюльку и разомните его вилкой так, чтобы в нем не осталось крупных комков. Разбейте в него яйца, всыпьте сахар и тщательно все перемешайте. Лучше не использовать слишком сухой или слишком влажный творог, иначе сырники будут разваливаться в процессе приготовления.\n",
      "\n",
      "Всыпьте в творог 5 столовых ложек (с горкой) муки и тщательно перемешайте. Можно добавить немного больше муки, сырники получатся тогда более плотными. Или муки можно добавить чуть меньше, и тогда сырники будут нежнее. В итоге у вас должна получиться однородная масса, из которой можно будет лепить сырники.\n",
      "\n",
      "Поставьте сковороду на средний огонь и налейте в нее подсолнечное масло.\n",
      "\n",
      "Насыпьте на тарелку немного муки. Слепите несколько небольших шариков из получившейся творожной массы и положите их на тарелку. Лучше лепить разом 4–5 шариков — столько, сколько поместится одновременно на сковороду. Затем по очереди обкатывайте творожные шарики в муке, плющите их в небольшие лепешки (они не должны быть слишком тонкие) и выкладывайте на сковороду.\n",
      "\n",
      "Обжаривайте сырники 1–2 минуты до появления золотистой корочки. Затем переверните их на другую сторону и также обжарьте до золотистого состояния.\n",
      "\n",
      "Повторяйте, пока творог не закончится.\n",
      "\n",
      "\n",
      "------------------------\n",
      "['полож', 'ве', 'творог', 'кастрюльк', 'разомн', 'вилк', 'нем', 'оста', 'крупн', 'комк', 'разб', 'яйц', 'всыпьт', 'сахар', 'тщательн', 'перемеша', 'лучш', 'использова', 'слишк', 'сух', 'слишк', 'влажн', 'творог', 'инач', 'сырник', 'будут', 'развалива', 'процесс', 'приготовлен', 'всыпьт', 'творог', '5', 'столов', 'ложек', 'горк', 'мук', 'тщательн', 'перемеша', 'можн', 'добав', 'немн', 'мук', 'сырник', 'получат', 'плотн', 'ил', 'мук', 'добав', 'меньш', 'сырник', 'будут', 'нежн', 'в', 'итог', 'должн', 'получ', 'однородн', 'масс', 'котор', 'леп', 'сырник', 'поставьт', 'сковород', 'средн', 'огон', 'нал', 'подсолнечн', 'масл', 'насыпьт', 'тарелк', 'немн', 'мук', 'слеп', 'нескольк', 'небольш', 'шарик', 'получ', 'творожн', 'масс', 'полож', 'тарелк', 'лучш', 'леп', 'раз', '4–5', 'шарик', '—', 'стольк', 'скольк', 'помест', 'одновремен', 'сковород', 'зат', 'очеред', 'обкатыва', 'творожн', 'шарик', 'мук', 'плющ', 'небольш', 'лепешк', 'должн', 'слишк', 'тонк', 'выкладыва', 'сковород', 'обжарива', 'сырник', '1–2', 'минут', 'появлен', 'золотист', 'корочк', 'зат', 'переверн', 'друг', 'сторон', 'такж', 'обжарьт', 'золотист', 'состоян', 'повторя', 'пок', 'творог', 'законч']\n"
     ]
    }
   ],
   "source": [
    "example = loaded_data.iloc[1][\"manual\"]\n",
    "print(example)\n",
    "print(\"------------------------\")\n",
    "print(tokenize_sentence(example))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T21:23:29.640109900Z",
     "start_time": "2023-07-06T21:23:29.606543700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "# Определение гиперпараметров для каждого компонента модели\n",
    "tfidf_params = {\n",
    "    'vectorizer__max_features': [500, 1000, 2000],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "    'vectorizer__use_idf': [True, False],\n",
    "    # Другие гиперпараметры...\n",
    "}\n",
    "normalizer_params = {\n",
    "    'normalizer__norm': ['l1', 'l2'],  # Тип нормализации (l2 - евклидова норма)\n",
    "    # Другие гиперпараметры...\n",
    "}\n",
    "\n",
    "random_forest_params = {\n",
    "    'model__estimator__n_estimators': [50, 100, 200],\n",
    "    'model__estimator__max_depth': [None, 5, 10],\n",
    "    'model__estimator__min_samples_split': [2, 5, 10],\n",
    "    # Другие гиперпараметры...\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T21:23:29.651583700Z",
     "start_time": "2023-07-06T21:23:29.640109900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "Xfeatures = tfidf.fit_transform(x_train).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T21:23:30.367529800Z",
     "start_time": "2023-07-06T21:23:29.651583700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "'binary_rel_clf = BinaryRelevance(MultinomialNB()) #our model\\nbinary_rel_clf.fit(Xfeatures, y_train)\\n'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''binary_rel_clf = BinaryRelevance(MultinomialNB()) #our model\n",
    "binary_rel_clf.fit(Xfeatures, y_train)\n",
    "'''\n",
    "#старая версия"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T21:23:30.387242900Z",
     "start_time": "2023-07-06T21:23:30.367529800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import\n",
    "\n",
    "sampler = MultilabelOverSampler()\n",
    "X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T21:40:27.936523Z",
     "start_time": "2023-07-06T21:40:27.929683Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 30 and the array at index 1 has size 0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m X_train_resampled, y_train_resampled \u001B[38;5;241m=\u001B[39m \u001B[43mrandom_oversampling\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mids_cat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2.0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[27], line 20\u001B[0m, in \u001B[0;36mrandom_oversampling\u001B[1;34m(X, y, classes, ratio)\u001B[0m\n\u001B[0;32m     18\u001B[0m     samples_to_duplicate\u001B[38;5;241m.\u001B[39mextend(selected_indices)\n\u001B[0;32m     19\u001B[0m X_resampled \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate((X, X[samples_to_duplicate]))\n\u001B[1;32m---> 20\u001B[0m y_resampled \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m[\u001B[49m\u001B[43msamples_to_duplicate\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X_resampled, y_resampled\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mconcatenate\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 30 and the array at index 1 has size 0"
     ]
    }
   ],
   "source": [
    "X_train_resampled, y_train_resampled = random_oversampling(x_train, y_train, ids_cat, ratio=2.0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-06T21:40:29.749957200Z",
     "start_time": "2023-07-06T21:40:29.661641400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline([\n",
    "    (\"vectorizer\",  TfidfVectorizer(tokenizer=lambda x: tokenize_sentence(x, remove_stop_words=True))),\n",
    "    (\"normalizer\", Normalizer()),\n",
    "    (\"model\", RandomForestClassifier())\n",
    "]\n",
    ")\n",
    "model_pipeline.fit(x_train, y_train)\n",
    "accuracy_score(y_test,model_pipeline.predict(x_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_text = \"Выпечка\"\n",
    "\n",
    "model_pipeline.predict_proba([test_text])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy_score(y_test,model_pipeline.predict(x_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precision_score(average='weighted', y_true=y_test, y_pred=model_pipeline.predict(x_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recall_score(average='weighted', y_true=y_test, y_pred=model_pipeline.predict(x_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_score = model_pipeline.predict_proba(x_test)\n",
    "print(y_score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(y_score[:, 0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_test['1']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# precision recall curve\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "\n",
    "for i in range(len(ids_cat)):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_true=y_test[ids_cat[i]],\n",
    "                                                        probas_pred=y_score[:, i])\n",
    "    plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))\n",
    "\n",
    "plt.xlabel(\"recall\")\n",
    "plt.ylabel(\"precision\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"precision vs. recall curve\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#оценка гиперпараметров\n",
    "\n",
    "grid_search = RandomizedSearchCV(model_pipeline, param_distributions={\n",
    "    **tfidf_params,\n",
    "    **normalizer_params,\n",
    "    **random_forest_params\n",
    "}, scoring='f1', cv=5, n_iter=2)\n",
    "\n",
    "grid_search.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict_prob(x_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
